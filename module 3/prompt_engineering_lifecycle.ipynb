{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Fetching pages: 100%|##########| 197/197 [00:39<00:00,  4.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To set up tracing to LangSmith using @traceable, first ensure that the LANGSMITH_TRACING environment variable is set to 'true', and the LANGSMITH_API_KEY is set to your API key. Then, simply decorate any function you want to trace with the @traceable decorator in your Python code. Make sure to use the await keyword when calling any synchronous functions wrapped in @traceable to log the traces correctly.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app import langsmith_rag\n",
    "\n",
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"To set up tracing to LangSmith using the @traceable decorator, first ensure you have set the LANGSMITH_TRACING environment variable to 'true' and LANGSMITH_API_KEY to your API key. Next, decorate any function in your Python code with @traceable. This allows traces to be logged when the function is called.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset to evaluate this particular step of our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'Technical Questions' already exists, using existing dataset.\n",
      "Examples added successfully.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithConflictError\n",
    "\n",
    "example_dataset = [\n",
    "    (\n",
    "        \"How do I set up tracing to LangSmith with @traceable?\",\n",
    "        \"\"\" 2. Log a trace​\\nOnce you've set up your environment, you can call LangChain runnables as normal.\\nLangSmith will infer the proper tracing config:\\n\\nHow to log and view traces to LangSmith | 🦜️🛠️ LangSmith\\n\\n2. Log a trace​\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingAnnotate code for tracingOn this pageAnnotate code for tracing\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable​\\nLangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript.\\nnoteThe LANGSMITH_TRACING environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\\nTo log traces to a different project, see this section. \\n\\n\"\"\",\n",
    "        \"To set up tracing to LangSmith using the `@traceable` decorator in Python, first ensure that you have the `LANGSMITH_TRACING` environment variable set to 'true' and the `LANGSMITH_API_KEY` set to your API key. Then, you can simply decorate your functions like this:\\n\\n```python\\nfrom langsmith import traceable\\n\\n@traceable\\ndef my_function():\\n    # Your code here\\n    pass\\n```\\n\\nThis will log traces for `my_function` automatically once the necessary environment variables are configured.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How can I use the tracing context manager?\",\n",
    "        \"\"\"Use the trace context manager (Python only)​\\nIn Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\\n\\nYou want to log traces for a specific block of code.\\nYou want control over the inputs, outputs, and other attributes of the trace.\\nIt is not feasible to use a decorator or wrapper.\\nAny or all of the above.\\n\\nIn some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.\\nRecently changed behaviorDue to a number of asks for finer-grained control of tracing using the trace context manager,\\nwe changed the behavior of with trace to honor the LANGSMITH_TRACING environment variable in version 0.1.95 of the Python SDK. You can find more details in the release notes.\\nThe recommended way to disable/enable tracing without setting environment variables is to use the with tracing_context context manager, as shown in the example below.\\nPythonTypeScriptThe recommended way to do this in Python is to use the tracing_context context manager. This works for both code annotated with traceable and code within the trace context manager.\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingTroubleshoot trace nestingOn this pageTroubleshoot trace nesting\\nWhen tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.\\nIf you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known \\\"edge cases\\\".\\nPython​\\nThe following outlines common causes for \\\"split\\\" traces when building with python.\\nContext propagation using asyncio​\\nWhen using async calls (especially with streaming) in Python versions < 3.11, you may encounter issues with trace nesting. This is because Python's asyncio only added full support for passing context in version 3.11.\\nWhy​\\nLangChain and LangSmith SDK use contextvars to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), asyncio tasks lack proper contextvar support, which can lead to disconnected traces.\\nTo resolve​\\n\\nContext propagation using threading​\\nIt's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib ThreadPoolExecutor by default breaks tracing.\\nWhy​\\nPython's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:\\nTo resolve​\\n\\n\\nUsing LangSmith's ContextThreadPoolExecutor\\nLangSmith provides a ContextThreadPoolExecutor that automatically handles context propagation:\\nfrom langsmith.utils import ContextThreadPoolExecutorfrom langsmith import traceable@traceabledef outer_func():    with ContextThreadPoolExecutor() as executor:        inputs = [1, 2]        r = list(executor.map(inner_func, inputs))@traceabledef inner_func(x):    print(x)outer_func()\\n\\n\\nManually providing the parent run tree\\nAlternatively, you can manually pass the parent run tree to the inner function:\\nfrom langsmith import traceable, get_current_run_treefrom concurrent.futures import ThreadPoolExecutor@traceabledef outer_func():    rt = get_current_run_tree()    with ThreadPoolExecutor() as executor:        r = list(            executor.map(                lambda x: inner_func(x, langsmith_extra={\\\"parent\\\": rt}), [1, 2]            )        )@traceabledef inner_func(x):    print(x)outer_func()\\nIn this approach, we use get_current_run_tree() to obtain the current run tree and pass it to the inner function using the langsmith_extra parameter. \\n\\n \"\"\",\n",
    "        \"You can use the tracing context manager by wrapping the code you want to trace with the `with trace_context:` statement. Here's a simple example:\\n\\n```python\\nfrom langsmith import trace_context\\n\\nwith trace_context:\\n    # Your traceable code here\\n    print(\\\"Tracing this block of code.\\\")\\n``` \\n\\nThis will log traces to LangSmith for the specified code block.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How can I use RunTree?\",\n",
    "        \"\"\"PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-4o-mini\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});await pipeline.postRun();// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI Call\\\",\\n\\nPythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})await pipeline.postRun();# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-3.5-turbo\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI\\n\\nAlternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function.\\n\\nRunTree({  run_type: \\\"llm\\\",  name: \\\"OpenAI Call RunTree\\\",  inputs: { messages },  tags: [\\\"my-tag\\\"],  extra: {metadata: {\\\"my-key\\\": \\\"my-value\\\"}}})await rt.postRun();const chatCompletion = await client.chat.completions.create({  model: \\\"gpt-3.5-turbo\\\",  messages: messages,});// End and submit the runawait rt.end(chatCompletion)await rt.patchRun()from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([(\\\"system\\\", \\\"You are a helpful AI.\\\"),(\\\"user\\\", \\\"{input}\\\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}})# Tags and metadata can also be passed at runtimechain.invoke({\\\"input\\\": \\\"What is the meaning of life?\\\"}, {\\\"tags\\\": [\\\"shared-tags\\\"], \\\"metadata\\\": {\\\"shared-key\\\": \\\"shared-value\\\"}})import { ChatOpenAI } from \\\"@langchain/openai\\\";import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";import { StringOutputParser } from \\\"@langchain/core/output_parsers\\\";const prompt = ChatPromptTemplate.fromMessages([[\\\"system\\\", \\\"You are a helpful AI.\\\"],[\\\"user\\\", \\\"{input}\\\"]])const model = new ChatOpenAI({ modelName: \\\"gpt-3.5-turbo\\\" });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}});// Tags and metadata can also be \\n\\n\"\"\",\n",
    "        \"You can use `RunTree` by creating a pipeline where you define its name, type, and inputs. For example:\\n\\n```python\\nfrom langsmith.run_trees import RunTree\\n\\npipeline = RunTree(name=\\\"My Pipeline\\\", run_type=\\\"chain\\\", inputs={\\\"question\\\": \\\"Your question here\\\"})\\n```\\n\\nYou can then create child runs and manage them accordingly within the pipeline.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Technical Questions\"\n",
    "\n",
    "# Try to create the dataset, or fetch existing one if it already exists\n",
    "try:\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Technical questions about LangSmith\"\n",
    "    )\n",
    "    print(f\"Dataset '{dataset_name}' created.\")\n",
    "except LangSmithConflictError:\n",
    "    # Dataset already exists, fetch it from the list\n",
    "    datasets = client.list_datasets()\n",
    "    dataset = next(ds for ds in datasets if ds.name == dataset_name)\n",
    "    print(f\"Dataset '{dataset_name}' already exists, using existing dataset.\")\n",
    "\n",
    "# Prepare inputs and outputs\n",
    "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
    "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
    "\n",
    "# Add examples\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")\n",
    "\n",
    "print(\"Examples added successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our Application to use Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pretty much define the same RAG application as before - with one crucial improvement.\n",
    "\n",
    "Instead of pulling our `RAG_PROMPT` from utils.py, we're going to connect to the Prompt Hub in LangSmith.\n",
    "\n",
    "Let's add the code snippet that will pull down our prompt that we just iterated on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'rag_with_code', 'lc_hub_commit_hash': 'bb5c444ac1c8cf2bce0d5353876e2cf3def3dbbd118bbbb426d7a923665bb882'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the latest question in the conversation. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\\n\\nWhenever possible provide a python code example to held the user get started\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Context: {context}\\nQuestion: {question}'), additional_kwargs={})]) middle=[] last=RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x2af0cca10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x106cd14c0>, root_client=<openai.OpenAI object at 0x2ae911e50>, root_async_client=<openai.AsyncOpenAI object at 0x2af0ccb60>, model_name='gpt-4o-mini', temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), kwargs={}, config={}, config_factories=[])\n"
     ]
    }
   ],
   "source": [
    "# 1. Install python-dotenv if not already installed\n",
    "# pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "\n",
    "# Load variables from your .env file\n",
    "load_dotenv(dotenv_path=\".env\")  # adjust path if your .env is elsewhere\n",
    "\n",
    "# Retrieve the key from the environment\n",
    "api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"LANGSMITH_API_KEY not found in environment variables!\")\n",
    "\n",
    "# Initialize the client\n",
    "client = Client(api_key=api_key)\n",
    "\n",
    "# Now you can pull the prompt\n",
    "prompt = client.pull_prompt(\"rag_with_code\", include_model=True)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langsmith import traceable\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "APP_VERSION = 1.0\n",
    "\n",
    "# TODO: Remove this hard-coded prompt and replace it with Prompt Hub\n",
    "\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    embd = OpenAIEmbeddings()\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    # Otherwise, index LangSmith documents and create new vector store\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    # TODO: Let's use our prompt pulled from Prompt Hub instead of manually formatting here!\n",
    "    \n",
    "    formatted_prompt = prompt.invoke({\"context\":formatted_docs, \"question\": question})\n",
    "    messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "    return call_openai(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_openai(messages: List[dict]) -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    documents = retrieve_documents(question)\n",
    "    \n",
    "    # Step 2: Format documents for the prompt\n",
    "    formatted_docs = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # Step 3: Invoke the prompt from Prompt Hub\n",
    "    formatted_prompt = prompt.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    # Step 4: Ensure it's in the correct format for OpenAI\n",
    "    if isinstance(formatted_prompt, AIMessage):\n",
    "        formatted_prompt = [formatted_prompt]  # wrap in list of messages\n",
    "    elif isinstance(formatted_prompt, str):\n",
    "        formatted_prompt = [AIMessage(content=formatted_prompt)]  # convert string to AIMessage\n",
    "    \n",
    "    # Step 5: Convert to OpenAI format\n",
    "    messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "    \n",
    "    # Step 6: Call OpenAI\n",
    "    response = call_openai(messages)\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To set up tracing to LangSmith using the `@traceable` decorator in Python, follow these steps:\\n\\n1. Ensure that you have the necessary environment variables configured:\\n   - Set the `LANGSMITH_TRACING` environment variable to \\'true\\'.\\n   - Set the `LANGSMITH_API_KEY` to your actual API key.\\n\\n2. Decorate your function with `@traceable`.\\n\\nHere\\'s a basic example demonstrating this setup:\\n\\n```python\\nimport os\\nfrom langsmith import traceable\\n\\n# Set up environment variables\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = \"your_api_key\"  # Replace with your actual API key\\n\\n@traceable\\ndef my_function():\\n    # Your function logic here\\n    print(\"Executing my_function\")\\n    # More code can go here\\n\\n# Call the function to see it in action\\nmy_function()\\n```\\n\\n### Key Points:\\n- Make sure you replace `\"your_api_key\"` with your actual LangSmith API key.\\n- The `@traceable` decorator will enable tracing for the decorated function, allowing you to monitor and log its execution as specified by LangSmith.\\n- You can run your script, and depending on how LangSmith is set up, you should see the tracing data get collected and sent to their service. \\n\\nMake sure you have the required `langsmith` library installed in your Python environment. If you need additional configurations or features from LangSmith, refer to their official documentation.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'To set up tracing to LangSmith using the `@traceable` decorator, you\\'ll need to ensure a couple of environment variables are configured and then apply the decorator to the functions you wish to trace. Here\\'s how to do it step-by-step:\\n\\n1. **Set Environment Variables**: Make sure you have the following environment variables set:\\n   - `LANGSMITH_TRACING`: This should be set to `\\'true\\'` to enable tracing.\\n   - `LANGSMITH_API_KEY`: This should be set with your actual API key for LangSmith to authenticate your requests.\\n\\n   You can set these environment variables in your shell as follows (for Unix-based systems):\\n   ```bash\\n   export LANGSMITH_TRACING=true\\n   export LANGSMITH_API_KEY=\"your_api_key_here\"\\n   ```\\n\\n   For Windows PowerShell, you would do:\\n   ```powershell\\n   $env:LANGSMITH_TRACING=\"true\"\\n   $env:LANGSMITH_API_KEY=\"your_api_key_here\"\\n   ```\\n\\n2. **Use the `@traceable` Decorator**: Once the environment is configured, you can use the `@traceable` decorator from the LangSmith library. Here is a minimal example:\\n\\n   ```python\\n   from langsmith import traceable\\n\\n   @traceable\\n   def sample_function():\\n       print(\"This function is being traced.\")\\n\\n   sample_function()\\n   ```\\n\\nWhen you run `sample_function()`, it will automatically be traced and relevant data will be sent to LangSmith according to your setup.\\n\\nRemember to check the LangSmith documentation for any additional configuration or usage details that may apply to your specific case.'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
